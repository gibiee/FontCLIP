{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading JIT archive /home/yuki/.cache/clip/ViT-B-32.pt\n",
      "120\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "import clip\n",
    "import os\n",
    "from utils.init_model import load_model, preprocess\n",
    "from utils.initialize_font_data import (\n",
    "    retrieve_font_path,\n",
    "    inclusive_attributes,\n",
    "    all_gray_scale_image_file_dir,\n",
    "    cj_font_dir,\n",
    "    font_dir,\n",
    "    train_json_path,\n",
    "    validation_json_path,\n",
    "    test_json_path,\n",
    "    validation_font_names,\n",
    "    all_json,\n",
    "    fox_text,\n",
    "    fox_text_four_lines,\n",
    ")\n",
    "from utils.evaluate_tools import (\n",
    "    generate_all_attribute_embedded_prompts,\n",
    "    user_attribute_choices_count,\n",
    "    compare_two_fonts,\n",
    "    evaluate_attribute_comparison_task,\n",
    "    evaluate_similarity_comparison_task,\n",
    "    user_similarity_choices,\n",
    ")\n",
    "\n",
    "# If using GPU then use mixed precision training.\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Must set jit=False for training\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# count font number\n",
    "train_font_num = len(list(json.load(open(train_json_path, \"r\")).keys()))\n",
    "print(train_font_num)\n",
    "\n",
    "validation_font_num = len(list(json.load(open(validation_json_path, \"r\")).keys()))\n",
    "print(validation_font_num)\n",
    "\n",
    "test_font_num = len(list(json.load(open(test_json_path, \"r\")).keys()))\n",
    "print(test_font_num)\n",
    "\n",
    "font_names = list(all_json.keys())\n",
    "font_paths = [\n",
    "    retrieve_font_path(font_name, font_dir=font_dir) for font_name in font_names\n",
    "]\n",
    "# font_names = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(font_dir)]\n",
    "\n",
    "fox_text = fox_text\n",
    "\n",
    "# target_font_names = extract_font_name_from_dir()\n",
    "target_font_names = list(all_json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.lora_multiheadattention import LoRAConfig\n",
    "from utils.dataset import TestImageDataset, TestTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_dataset = TestImageDataset(\n",
    "    font_dir,\n",
    "    validation_json_path,\n",
    "    fox_text,\n",
    "    dump_image=True,\n",
    "    image_file_dir=all_gray_scale_image_file_dir,\n",
    "    preprocess=preprocess,\n",
    ")\n",
    "target_attributes = [\"angular\", \"monospace\", \"capitals\", \"serif\"]\n",
    "text_dataset = TestTextDataset(\n",
    "    target_attributes=target_attributes,\n",
    "    context_length=77,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_font_to_attributes = json.load(open(validation_json_path, \"r\"))\n",
    "val_font_to_attributes = {\n",
    "    font_name: [float(v) for a, v in attributes.items() if a in target_attributes]\n",
    "    for font_name, attributes in val_font_to_attributes.items()\n",
    "    if font_name in validation_font_names\n",
    "}\n",
    "ground_truth_attributes = torch.tensor([val_font_to_attributes[font_name] for font_name in validation_font_names]).T\n",
    "ground_truth_attributes = ground_truth_attributes.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 40)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.49,  33.49,  20.24,  92.26,  22.21,  15.75,  50.42,  35.11,\n",
       "         72.65,  26.48,  24.37,  41.02,  29.62,  74.69,  23.62,  24.18,\n",
       "         38.97,  32.17,  20.18,  58.97,  52.11,  68.08,  34.52,  39.88,\n",
       "         95.05,  24.64,  76.59,  30.56,  46.88,  21.16,  38.32,  15.69,\n",
       "         32.67,  20.92,  23.61,  32.41,  49.54,  14.98,  48.24,  51.87],\n",
       "       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  , 100.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "        100.  ,   0.  , 100.  ,   0.  , 100.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n",
       "       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  , 100.  ,   0.  ,\n",
       "          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n",
       "          0.  , 100.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n",
       "       [  9.21,  79.52,   9.36,  80.94,  10.2 ,   9.77,  81.12,   9.7 ,\n",
       "          9.77,   9.43,   9.63,  17.12,   9.9 ,  75.05,   9.18,   9.1 ,\n",
       "         81.25,  82.83,   9.73,  82.14,   5.48,  13.03,  10.2 ,   9.13,\n",
       "          8.73,  81.73,  87.91,  87.33,  85.71,  11.35,  85.99,   9.33,\n",
       "          9.63,  10.2 ,  10.06,  81.83,   9.64,   9.63,  15.91,   9.05]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config_vision = LoRAConfig(\n",
    "    r=256,\n",
    "    alpha=512.0,\n",
    "    bias=False,\n",
    "    learnable_alpha=False,\n",
    "    apply_q=True,\n",
    "    apply_k=True,\n",
    "    apply_v=True,\n",
    "    apply_out=True,\n",
    ")\n",
    "lora_config_text = LoRAConfig(\n",
    "    r=256,\n",
    "    alpha=1024.0,\n",
    "    bias=False,\n",
    "    learnable_alpha=False,\n",
    "    apply_q=True,\n",
    "    apply_k=True,\n",
    "    apply_v=True,\n",
    "    apply_out=True,\n",
    ")\n",
    "checkpoint_path = \"model_checkpoints/cv_20_0_task_for_validation_ViT-B_32_bce_lora_t-qkvo_256-1024.0_91011_batch64_aug250_lbound_of_scale0.35_max_attr_num_3_random_p_num_70000_geta0.2_use_negative_til1.0_lr2e-05-0.1_image_file_dir.pt\"\n",
    "tmp_model = load_model(\n",
    "    model,\n",
    "    checkpoint_path,\n",
    "    model_name=\"ViT-B/32\",\n",
    "    learnable_prompt=False,\n",
    "    learnable_vision=False,\n",
    "    precontext_length=48,\n",
    "    precontext_vision_length=0,\n",
    "    precontext_dropout_rate=0,\n",
    "    vpt_applied_layers=None,\n",
    "    use_oft_vision=False,\n",
    "    use_oft_text=False,\n",
    "    oft_config_vision=None,\n",
    "    oft_config_text=None,\n",
    "    inject_lora=True,\n",
    "    lora_config_vision=lora_config_vision,\n",
    "    lora_config_text=lora_config_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgfont-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
